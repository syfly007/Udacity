{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "24824560-0dba-4a71-8e35-ce73eb9649b6"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ebc32ea-8e64-4622-9ec7-8e1fa60b64e5"
    }
   },
   "source": [
    "![test](img-1.png)\n",
    "![test](img-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a6b44630-bd2e-46b9-a874-7029ac1dcb6b"
    }
   },
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3e483173-2226-4d50-bd63-633d3643fafe"
    }
   },
   "source": [
    "![test](img-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d540e9a3-9896-4c0a-aeb3-4d5f221e1aad"
    }
   },
   "source": [
    "每个神经元都会接受其他神经元的输入，然后决定是否传递神经脉冲\n",
    "![test](img-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3ab4c514-12bb-40d8-9b2a-bc188cf5bbbf"
    }
   },
   "source": [
    "## [Perceptron](http://note.youdao.com/noteshare?id=0a8b8abeffb9374936ca13b8c7e77de3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "af7a5a01-69b4-41ea-bb23-8fe89b793723"
    }
   },
   "source": [
    "# AND Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "1ed00751-0004-420e-9509-477eaeded1b7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -1.1                    0          Yes\n",
      "      0          1                  -0.1                    0          Yes\n",
      "      1          0                  -0.1                    0          Yes\n",
      "      1          1                   0.9                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -1.1\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7651fec6-9fa0-4331-993a-c8ac5a9c051d"
    }
   },
   "source": [
    "# OR Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "54f47273-09e6-4078-87fa-da364e41db1d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -0.9                    0          Yes\n",
      "      0          1                   0.1                    1          Yes\n",
      "      1          0                   0.1                    1          Yes\n",
      "      1          1                   1.1                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -0.9\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, True, True, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ee9bd2d2-6479-4b9d-b3e3-358076a0f27d"
    }
   },
   "source": [
    "The OR perceptron is very similar to an AND perceptron. In the image below, the OR perceptron has the same line as the AND perceptron, except the line is shifted down. What can you do to the weights and/or bias to achieve this? Use the following AND perceptron to create an OR Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f72ded58-0c12-457b-8e95-ce3fceca6ec4"
    }
   },
   "source": [
    "![test](img-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1315384e-25a0-4a73-ac9a-e3bc270bdfff"
    }
   },
   "source": [
    "What are two ways to go from an AND perceptron to an OR perceptron?\n",
    ">* Increase the weights\n",
    ">* Decrease the magnitude of the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "87a883cd-7098-450b-a512-0a4422a55b07"
    }
   },
   "source": [
    "# NOT Perceptron\n",
    "\n",
    "Unlike the other perceptrons we looked at, the NOT operations only cares about one input. The operation returns a 0 if the input is 1 and a 1 if it's a 0. The other inputs to the perceptron are ignored.\n",
    "\n",
    "In this quiz, you'll set the weights (weight1, weight2) and bias bias to the values that calculate the NOT operation on the second input and ignores the first input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0f77a547-c32b-4ec7-a759-bf5349edcdc4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   0.0                    1          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                   0.0                    1          Yes\n",
      "      1          1                  -1.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -1.0\n",
    "bias = 0.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "61a83d6e-0fef-466a-a57f-b588efdfa52f"
    }
   },
   "source": [
    "# XOR Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c7bec236-b8e9-4079-a8e7-4113790b0b9c"
    }
   },
   "source": [
    "<div align=center><img width=\"40%\" height=\"40%\" src=\"img-6.png\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c6171a9c-7215-4dc4-9da9-c4cdf0a918fe"
    }
   },
   "source": [
    "An XOR perceptron is a logic gate that outputs 0 if the inputs are the same and 1 if the inputs are different. Unlike previous perceptrons, this graph isn't linearly separable. To handle more complex problems like this, we can chain perceptrons together.\n",
    "\n",
    "Let's build a neural network from the AND, NOT, and OR perceptrons to create XOR logic. Let's first go over what a neural network looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9bb0026c-fdd2-4237-bd60-0b9f2e7c626b"
    }
   },
   "source": [
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"img-7.png\"/></div>\n",
    "\n",
    "The above neural network contains 4 perceptrons, A, B, C, and D. The input to the neural network is from the first node. The output comes out of the last node. The weights are based on the line thickness between the perceptrons. Any link between perceptrons with a low weight, like A to C, you can ignore. For perceptron C, you can ignore all input to and from it. For simplicity we won't be showing bias, but it's still in the neural network.\n",
    "\n",
    "## Quiz\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"img-8.png\"/></div>\n",
    "\n",
    ">* A = NOT\n",
    ">* B = AND\n",
    ">* C = OR\n",
    "\n",
    "The neural network above calculates XOR. Each perceptron is a logic operation of OR, AND, Passthrough, or NOT. The Passthrough operation just passes it's input to the output. However, the perceptrons A , B, and C don't indicate their operation. In the following quiz, set the correct operations for the three perceptrons to calculate XOR.\n",
    "\n",
    "Note: Any line with a low weight can be ignored.\n",
    "\n",
    "You've seen that a perceptron can solve linearly separable problems. Solving more complex problems, you use more perceptrons. You saw this by calculating AND, OR, NOT, and XOR operations using perceptrons. These operations can be used to create any computer program. With enough data and time, a neural network can solve any problem that a computer can calculate. However, you don't build a Twitter using a neural network. A neural network is like any tool, you have to know when to use it.\n",
    "\n",
    "The power of a neural network isn't building it by hand, like we were doing. It's the ability to learn from examples. In the next few sections, you'll learn how a neural networks sets it's own weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a259dc8c-cb1c-4f82-b283-75880ae43a41"
    }
   },
   "source": [
    "# The simplest neural network\n",
    "So far you've been working with perceptrons where the output is always one or zero. The input to the output unit is passed through an activation function, f(h), in this case, the step function.\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"img-9.png\"/></div>\n",
    "\n",
    "The output unit returns the result of f(h), where h is the input to the output unit:\n",
    "$$h=\\sum_{i}w_{i}x_{i} + b$$\n",
    "\n",
    "The diagram below shows a simple network. The linear combination of the weights, inputs, and bias form the input h, which passes through the activation function f(h), giving the final output of the perceptron, labeled y.\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/e429472f-a8bf-411a-87e5-6abf1223a725\"/></div>\n",
    "<center>Diagram of a simple neural network. Circles are units, boxes are operations</center>\n",
    "\n",
    "The cool part about this architecture, and what makes neural networks possible, is that the activation function, f(h) can be any function, not just the step function shown earlier.\n",
    "\n",
    "For example, if you let f(h)=h, the output will be the same as the input. Now the output of the network is\n",
    "\n",
    "$$h=\\sum_{i}w_{i}x_{i} + b$$\n",
    "This equation should be familiar to you, it's the same as the linear regression model!\n",
    "\n",
    "Other activation functions you'll see are the logistic (often called the sigmoid), tanh, and softmax functions. We'll mostly be using the sigmoid function for the rest of this lesson:\n",
    "\n",
    "$$sigmoid(x)=1/(1+e^{−x})$$\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/e43896bc-5796-4c40-8312-f162863b142e\"/></div>\n",
    "<center>The sigmoid function</center>\n",
    "\n",
    "The sigmoid function is bounded between 0 and 1, and as an output can be interpreted as a probability for success. It turns out, again, using a sigmoid as the activation function results in the same formulation as logistic regression.\n",
    "\n",
    "This is where it stops being a perceptron and begins being called a neural network. In the case of simple networks like this, neural networks don't offer any advantage over general linear models such as logistic regression.\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"img-8.png\"/></div>\n",
    "<center>As you saw earlier in the XOR perceptron, stacking units lets us model linearly inseparable data.</center>\n",
    "\n",
    "But, as you saw with the XOR perceptron, stacking units will let you model linearly inseparable data, impossible to do with regression models.\n",
    "\n",
    "Once you start using activation functions that are continuous and differentiable, it's possible to train the network using **gradient descent**, which you'll learn about next.\n",
    "\n",
    "## Simple network exercise\n",
    "\n",
    "Below you'll use Numpy to calculate the output of a simple network with two input nodes and one output node with a sigmoid activation function. Things you'll need to do:\n",
    "\n",
    "Implement the sigmoid function.\n",
    "Calculate the output of the network.\n",
    "As a reminder, the sigmoid function is\n",
    "\n",
    "$$sigmoid(x)=1/(1+e^{−x})$$\n",
    "\n",
    "For the exponential, you can use Numpy's exponential function, np.exp.\n",
    "\n",
    "And the output of the network is\n",
    "\n",
    "$$y=f(h)=sigmoid(\\sum_{i}w_{i}x_{i} + b)$$\n",
    "\n",
    "For the weights sum, you can do a simple element-wise multiplication and sum, or use Numpy's dot product function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "105dc666-8c84-413f-9161-bd1af1c7a577"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0.432907095035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.e**(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = sigmoid(np.dot(inputs,weights)+bias)\n",
    "\n",
    "print('Output:')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9c63fe37-6a73-432f-a102-afe02a462b9f"
    }
   },
   "source": [
    "# Gradient Descent\n",
    "## Learning weights\n",
    "\n",
    "You've seen how you can use perceptrons for AND and XOR operations, but there we set the weights by hand. What if you want to perform an operation, such as predicting college admission, but don't know the correct weights? You'll need to learn the weights from example data, then use those weights to make the predictions.\n",
    "\n",
    "To figure out how we're going to find these weights, start by thinking about the goal. We want the network to make predictions as close as possible to the real values. To measure this, we need a metric of how wrong the predictions are, the error. A common metric is the sum of the squared errors (SSE):\n",
    "\n",
    "$$E=\\dfrac {1}{2}\\sum _{\\mu }\\sum _{j}\\left[ y^{\\mu }_{j}-\\widehat {y}^{\\mu }_{j}\\right] ^{2}$$\n",
    "\n",
    "where \n",
    "​y\n",
    "​^\n",
    "​​  is the prediction and y is the true value, and you take the sum over all output units j and another sum over all data points μ. This might seem like a really complicated equation at first, but it's fairly simple once you understand the symbols and can say what's going on in words.\n",
    "\n",
    "First, the inside sum over j. This variable j represents the output units of the network. So this inside sum is saying for each output unit, find the difference between the true value y and the predicted value from the network \n",
    "​y\n",
    "​^\n",
    "​​ , then square the difference, then sum up all those squares.\n",
    "\n",
    "Then the other sum over μ is a sum over all the data points. So, for each data point you calculate the inner sum of the squared differences for each output unit. Then you sum up those squared differences for each data point. That gives you the overall error for all the output predictions for all the data points.\n",
    "The SSE is a good choice for a few reasons. The square ensures the error is always positive and larger errors are penalized more than smaller errors. Also, it makes the math nice, always a plus.\n",
    "\n",
    "Remember that the output of a neural network, the prediction, depends on the weights\n",
    "\n",
    "$$\\widehat {y}^{\\mu }_{j}=f( \\sum _{i}w_{ij}x^{\\mu }_{i}) $$\n",
    "\n",
    "and accordingly the error depends on the weights\n",
    "\n",
    "$$E=\\dfrac {1}{2}\\sum _{\\mu }\\sum _{j}\\left[ y^{\\mu }_{j}-f( \\sum _{i}w_{ij}x^{\\mu }_{i})\\right] ^{2}$$\n",
    "\n",
    "We want the network's prediction error to be as small as possible and the weights are the knobs we can use to make that happen. Our goal is to find weights w\n",
    "​ij\n",
    "​​  that minimize the squared error E. To do this with a neural network, typically you'd use gradient descent.\n",
    "\n",
    "## Enter Gradient Descent\n",
    "\n",
    "As Luis said, with gradient descent, we take multiple small steps towards our goal. In this case, we want to change the weights in steps that reduce the error. Continuing the analogy, the error is our mountain and we want to get to the bottom. Since the fastest way down a mountain is in the steepest direction, the steps taken should be in the direction that minimizes the error the most. We can find this direction by calculating the gradient of the squared error.\n",
    "\n",
    "Gradient is another term for rate of change or slope. If you need to brush up on this concept, check out Khan Academy's great lectures on the topic.\n",
    "\n",
    "To calculate a rate of change, we turn to calculus, specifically derivatives. A derivative of a function f(x) gives you another function f\n",
    "​′\n",
    "​​ (x) that returns the slope of f(x) at point x. For example, consider f(x)=x\n",
    "​2\n",
    "​​ . The derivative of x\n",
    "​2\n",
    "​​  is f\n",
    "​′\n",
    "​​ (x)=2x. So, at x=2, the slope is f\n",
    "​′\n",
    "​​ (2)=4. Plotting this out, it looks like:\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/1fce3562-a676-4a58-81c4-772db2ed6a5d\"/></div>\n",
    "<center>Example of a gradient</center>\n",
    "\n",
    "The gradient is just a derivative generalized to functions with more than one variable. We can use calculus to find the gradient at any point in our error function, which depends on the input weights. You'll see how the gradient descent step is derived on the next page.\n",
    "Below I've plotted an example of the error of a neural network with two inputs, and accordingly, two weights. You can read this like a topographical map where points on a contour line have the same error and darker contour lines correspond to larger errors.\n",
    "\n",
    "At each step, you calculate the error and the gradient, then use those to determine how much to change each weight. Repeating this process will eventually find weights that are close to the minimum of the error function, the block dot in the middle.\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/bf8d1191-6d28-4940-9625-b033ca0e5720\"/></div>\n",
    "<center>Gradient descent steps to the lowest error</center>\n",
    "\n",
    "## Caveats\n",
    "\n",
    "Since the weights will just go where ever the gradient takes them, they can end up where the error is low, but not the lowest. These spots are called local minima. If the weights are initialized with the wrong values, gradient descent could lead the weights into a local minimum, illustrated below.\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/53b0f168-9dce-4a56-9601-2a965e171392\"/></div>\n",
    "<center>Gradient descent leading into a local minimum</center>\n",
    "\n",
    "There are methods to avoid this, such as using [momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "735230e5-94f4-475f-80f4-7be267493035"
    }
   },
   "source": [
    "## The Code\n",
    "\n",
    "From before we saw that one weight update can be calculated as:\n",
    "\n",
    "$$\\Delta W_{i}=\\eta \\delta x_{i}$$\n",
    "\n",
    "with the error term δ as\n",
    "\n",
    "$$\\delta =\\left( y-\\widehat {y}\\right) f'\\left( h\\right) =f'\\left( \\sum w_{i}x_{i}\\right) $$\n",
    "\n",
    "Remember, in the above equation $\\left( y-\\widehat {y}\\right)$ is the output error, and $f'\\left( h\\right)$ refers to the derivative of the activation function, f(h). We'll call that derivative the output gradient.\n",
    "\n",
    "Now I'll write this out in code for the case of only one output unit. We'll also be using the sigmoid as the activation function f(h)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "db63c7dd-03d1-43e1-9989-3f76a4ceed83"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.689974481128\n",
      "Amount of Error:\n",
      "-0.189974481128\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "# Note: The sigmoid_prime function calculates sigmoid(h) twice,\n",
    "#       but you've already calculated it once. You can make this\n",
    "#       code more efficient by calculating the derivative directly\n",
    "#       rather than calling sigmoid_prime, like this:\n",
    "# error_term = error * nn_output * (1 - nn_output)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3c2e62b-e799-41b1-84b3-bdedd9d393f4"
    }
   },
   "source": [
    "# Implementing gradient descent\n",
    "\n",
    "Okay, now we know how to update our weights:\n",
    "\n",
    "$$\\Delta W_{i}=\\eta \\delta x_{i}$$\n",
    "\n",
    "You've seen how to implement that for a single update, but how do we translate that code to calculate many weight updates so our network will learn?\n",
    "\n",
    "As an example, I'm going to have you use gradient descent to train a network on graduate school admissions data (found at http://www.ats.ucla.edu/stat/data/binary.csv. This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest.\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/cae7de8d-35a6-4f53-9541-d839ffd4f09f\"/></div>\n",
    "\n",
    "The goal here is to predict if a student will be admitted to a graduate program based on these features. For this, we'll use a network with one output layer with one unit. We'll use a sigmoid function for the output unit activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "3eee9e6a-b2de-423a-8db7-dd40cc08dd5e"
    }
   },
   "source": [
    "## Data cleanup\n",
    "\n",
    "You might think there will be three input units, but we actually need to transform the data first. The rank feature is categorical, the numbers don't encode any sort of relative values. Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2. Instead, we need to use dummy variables to encode rank, splitting the data into four new columns encoded with ones or zeros. Rows with rank 1 have one in the rank 1 dummy column, and zeros in all other columns. Rows with rank 2 have one in the rank 2 dummy column, and zeros in all other columns. And so on.\n",
    "\n",
    "We'll also need to standardize the GRE and GPA data, which means to scale the values such they have zero mean and a standard deviation of 1. This is necessary because the sigmoid function squashes really small and really large inputs. The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too. Since the GRE and GPA values are fairly large, we have to be really careful about how we initialize the weights or the gradient descent steps will die off and the network won't train. Instead, if we standardize the data, we can initialize the weights easily and everyone is happy.\n",
    "\n",
    "This is just a brief run-through, you'll learn more about preparing data later. If you're interested in how I did this, check out the data_prep.py file in the programming exercise below.\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/0c580bc2-b0a9-4952-bfd0-f2ce6093efe8\"/></div>\n",
    "<center>Ten rows of the data after transformations.</center>\n",
    "Now that the data is ready, we see that there are six input features: gre, gpa, and the four rank dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "309c3d7c-aed4-4999-a2ed-6d102ca2120f"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "401ecf92-3890-4c92-bd82-b219ae5b7096"
    }
   },
   "outputs": [],
   "source": [
    "# print features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2e313dc4-9482-4a3c-904b-4bb43facd2d8"
    }
   },
   "source": [
    "## Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4565ae43-5494-4398-ae2c-405e654fa342"
    }
   },
   "source": [
    "We're going to make a small change to how we calculate the error here. Instead of the SSE, we're going to use the **mean** of the square errors (MSE). Now that we're using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, you'd need to use a quite small learning rate. Instead, we can just divide by the number of records in our data, m to take the average. This way, no matter how much data we use, our learning rates will typically be in the range of 0.01 to 0.001. Then, we can use the MSE (shown below) to calculate the gradient and the result is the same as before, just averaged instead of summed.\n",
    "\n",
    "$$E=\\dfrac {1}{2m}\\sum _{\\mu }\\left( y^{\\mu }-\\widehat {y}^{\\mu }\\right) ^{2}$$\n",
    "\n",
    "Here's the general algorithm for updating the weights with gradient descent:\n",
    "\n",
    ">* Set the weight step to zero:  $\\Delta w_{i}=0$\n",
    ">* For each record in the training data:\n",
    "    * Make a forward pass through the network, calculating the output \n",
    "$\\widehat {y} =f\\left( \\sum_{i} w_{i}x_{i}\\right) $\n",
    "    * Calculate the error term for the output unit, $ \\delta=(y− \\widehat {y}) * f'\\left( \\sum_{i} w_{i}x_{i}\\right)$\n",
    "     * Update the weight step  $ \\Delta w_{i} = \\Delta w_{i} + \\delta x_{i} $\n",
    ">* Update the weights $ w_{i} = w_{i} + \\eta \\Delta w_{i} / m $ where η is the learning rate and m is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    ">* Repeat for e epochs.\n",
    "\n",
    "You can also update the weights on each record instead of averaging the weight steps after going through all the records.\n",
    "\n",
    "Remember that we're using the sigmoid for the activation function, $f(h)=1/(1+e^{−h})$\n",
    "\n",
    "And the gradient of the sigmoid is $ f'(h)=f(h)(1−f(h)) $\n",
    "\n",
    "where h is the input to the output unit,\n",
    "\n",
    "$ h=\\sum_{i}w_{i}x_{i} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "154ec630-3a0c-4e8d-b0a7-3afa7b53bb5b"
    }
   },
   "source": [
    "## Implementing with Numpy\n",
    "\n",
    "For the most part, this is pretty straightforward with Numpy.\n",
    "\n",
    "First, you'll need to initialize the weights. We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is 1/√\n",
    "​n\n",
    "​\n",
    "​​  where n is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units.\n",
    "\n",
    "```python\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "```\n",
    "    \n",
    "Numpy provides a function that calculates the dot product of two arrays, which conveniently calculates h for us. The dot product multiplies two arrays element-wise, the first element in array 1 is multiplied by the first element in array 2, and so on. Then, each product is summed.\n",
    "\n",
    "```python\n",
    "# input to the output layer\n",
    "output_in = np.dot(weights, inputs)\n",
    "```\n",
    "\n",
    "And finally, we can update Δw\n",
    "​i\n",
    "​​  and w\n",
    "​i\n",
    "​​  by incrementing them with **weights += ... **which is shorthand for **weights = weights + ....**\n",
    "\n",
    "**Efficiency tip!**\n",
    "\n",
    "You can save some calculations since we're using a sigmoid here. For the sigmoid function, f\n",
    "​′\n",
    "​​ (h)=f(h)(1−f(h)). That means that once you calculate f(h), the activation of the output unit, you can use it to calculate the gradient for the error gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9d8cfc3c-1b7b-414b-9816-6de552987d1f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13052668,  0.18835906, -0.62977955,  0.13472121, -0.17116949,\n",
       "        0.245653  , -0.01506626,  0.16402559, -0.12377212,  0.13851491])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成符合正太分布的随机值\n",
    "# loc：float\n",
    "#     此概率分布的均值（对应着整个分布的中心centre）\n",
    "# scale：float\n",
    "#     此概率分布的标准差（对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高）\n",
    "# size：int or tuple of ints\n",
    "#     输出的shape，默认为None，只输出一个值\n",
    "\n",
    "np.random.normal(loc=0.0,scale=1/(10**.5),size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4dba093f-793b-475c-9a3e-500c6d9e6dba"
    }
   },
   "source": [
    "## Programming exercise\n",
    "Below, you'll implement gradient descent and train the network on the admissions data. Your goal here is to train the network until you reach a minimum in the mean square error (MSE) on the training set. You need to implement:\n",
    "\n",
    ">* The network output: <font color=red>output</font>.\n",
    ">* The output error: <font color=red>error</font>.\n",
    ">* The error term: <font color=red>error_term.</font>.\n",
    ">* Update the weight step: <font color=red>del_w +=</font>.\n",
    ">* Update the weights: <font color=red>weights +=</font>.\n",
    "\n",
    "After you've written these parts, run the training by pressing \"Test Run\". The MSE will print out, as well as the accuracy on a test set, the fraction of correctly predicted admissions.\n",
    "\n",
    "Feel free to play with the hyperparameters and see how it changes the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ad222687-d182-407d-9b47-9ead195d195b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.2560141778026995)\n",
      "('Train loss: ', 0.22155313121082948)\n",
      "('Train loss: ', 0.22877160615845615, '  WARNING - Loss Increasing')\n",
      "('Train loss: ', 0.22521184692945645)\n",
      "('Train loss: ', 0.24327139974821355, '  WARNING - Loss Increasing')\n",
      "('Train loss: ', 0.23010707188315396)\n",
      "('Train loss: ', 0.23755888156990543, '  WARNING - Loss Increasing')\n",
      "('Train loss: ', 0.22197068316143173)\n",
      "('Train loss: ', 0.22340943471591118, '  WARNING - Loss Increasing')\n",
      "('Train loss: ', 0.22302763143611634)\n",
      "Prediction accuracy: 0.744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(weights,x))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * sigmoid_der(output)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += learnrate*error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    #　少除以n\n",
    "#     weights += del_w/n_records\n",
    "    weights += del_w\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9173e3cf-ac24-48e4-a768-decaac7e7272"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.2656293235583206)\n",
      "('Train loss: ', 0.20824110661656858)\n",
      "('Train loss: ', 0.19944438291964234)\n",
      "('Train loss: ', 0.19713079092080757)\n",
      "('Train loss: ', 0.19626429019998226)\n",
      "('Train loss: ', 0.19587504950820817)\n",
      "('Train loss: ', 0.1956814955278593)\n",
      "('Train loss: ', 0.19557878002878454)\n",
      "('Train loss: ', 0.19552167219077843)\n",
      "('Train loss: ', 0.1954887690876647)\n",
      "Prediction accuracy: 0.718\n"
     ]
    }
   ],
   "source": [
    "#solution.py\n",
    "\n",
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Activation of the output unit\n",
    "        #   Notice we multiply the inputs and the weights here \n",
    "        #   rather than storing h as a separate variable \n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # The error, the target minus the network output\n",
    "        error = y - output\n",
    "\n",
    "        # The error term\n",
    "        #   Notice we calulate f'(h) here instead of defining a separate\n",
    "        #   sigmoid_prime function. This just makes it faster because we\n",
    "        #   can re-use the result of the sigmoid function stored in\n",
    "        #   the output variable\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "        # The gradient descent step, the error times the gradient times the inputs\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # Update the weights here. The learning rate times the \n",
    "    # change in weights, divided by the number of records to average\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0c97b68b-7942-46e0-81c5-878f98dae209"
    }
   },
   "source": [
    "# Multilayer Perceptrons \n",
    "\n",
    "## Implementing the hidden layer\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Below, we are going to walk through the math of neural networks in a multilayer perceptron. With multiple perceptrons, we are going to move to using vectors and matrices. To brush up, be sure to view the following:\n",
    "\n",
    "* Khan Academy's introduction to vectors.\n",
    "* Khan Academy's introduction to matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fa9e906c-d45b-4361-a978-333301bdeed7"
    }
   },
   "source": [
    "**Derivation**\n",
    "\n",
    "Before, we were dealing with only one output node which made the code straightforward. However now that we have multiple input units and multiple hidden units, the weights between them will require two indices: w\n",
    "​ij\n",
    "​​  where i denotes input units and j are the hidden units.\n",
    "\n",
    "For example, the following image shows our network, with its input units labeled x\n",
    "​1\n",
    "​​ ,x\n",
    "​2\n",
    "​​ , and x\n",
    "​3\n",
    "​​ , and its hidden nodes labeled h\n",
    "​1\n",
    "​​  and h\n",
    "​2:\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/c22ee73f-ff58-47da-a248-e319cbb046d2\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "84decc81-73a1-4c09-9669-96af9fd7bbfb"
    }
   },
   "source": [
    "The lines indicating the weights leading to h\n",
    "​1\n",
    "​​  have been colored differently from those leading to h\n",
    "​2\n",
    "​​  just to make it easier to read.\n",
    "\n",
    "Now to index the weights, we take the input unit number for the \n",
    "​i\n",
    "​​  and the hidden unit number for the \n",
    "​j\n",
    "​​ . That gives us\n",
    "\n",
    "w\n",
    "​11\n",
    "​​ \n",
    "\n",
    "for the weight leading from x\n",
    "​1\n",
    "​​  to h\n",
    "​1\n",
    "​​ , and\n",
    "\n",
    "w\n",
    "​12\n",
    "​​ \n",
    "\n",
    "for the weight leading from x\n",
    "​1\n",
    "​​  to h\n",
    "​2\n",
    "​​ .\n",
    "\n",
    "The following image includes all of the weights between the input layer and the hidden layer, labeled with their appropriate w\n",
    "​ij\n",
    "​​  indices:\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/ee96afdd-a46e-4be6-b6b2-39fc9c8ff2b5\"/></div>\n",
    "\n",
    "Before, we were able to write the weights as an array, indexed as w\n",
    "​i\n",
    "​​ .\n",
    "\n",
    "But now, the weights need to be stored in a **matrix**, indexed as w\n",
    "​ij\n",
    "​​ . Each **row** in the matrix will correspond to the weights **leading out** of a **single input unit**, and each **column** will correspond to the weights **leading in** to a **single hidden unit**. For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/6f15956b-0cf5-4c07-a7b0-db1c8db26653\"/></div>\n",
    "<center>Weights matrix for 3 input units and 2 hidden units</center>\n",
    "\n",
    "Be sure to compare the matrix above with the diagram shown before it so you can see where the different weights in the network end up in the matrix.\n",
    "\n",
    "To initialize these weights in Numpy, we have to provide the shape of the matrix. If features is a 2D array containing the input data:\n",
    "\n",
    "```python\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "This creates a 2D array (i.e. a matrix) named <font color=\"red\">weights_input_to_hidden</font> with dimensions <font color=\"red\">n_inputs</font> by <font color=\"red\">n_hidden</font>. Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit, h\n",
    "​j\n",
    "​​ , we need to calculate the following:\n",
    "$$h_{j} = \\sum_{i}w_{ij}x_{i} $$\n",
    "\n",
    "To do that, we now need to use **matrix multiplication**. If your linear algebra is rusty, I suggest taking a look at the suggested resources in the prerequisites section. For this part though, you'll only need to know how to multiply a matrix with a vector.\n",
    "\n",
    "In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, j=1, you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/82fcbaec-580b-42d1-b862-f18b3eae3c14\"/></div>\n",
    "<center>Calculating the input to the first hidden unit with the first column of the weights matrix.</center>\n",
    "\n",
    "$$h_{1} = x_{1}w_{11} + x_{2}w_{21} + x_{3}w_{31}$$\n",
    "\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.\n",
    "\n",
    "In Numpy, you can do this for all the inputs and all the outputs at once using <font color=\"red\">np.dot\n",
    "</font>\n",
    "\n",
    "```python\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "```\n",
    "\n",
    "You could also define your weights matrix such that it has dimensions **n_hidden** by **n_inputs** then multiply like so where the inputs form a column vector:\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/b0d94818-c8f0-4c81-a573-d5fbf775220c\"/>\n",
    "\n",
    "**Note**: The weight indices have changed in the above image and no longer match up with the labels used in the earlier diagrams. That's because, in matrix notation, the row index always precedes the column index, so it would be misleading to label them the way we did in the neural net diagram. Just keep in mind that this is the same weight matrix as before, but rotated so the first column is now the first row, and the second column is now the second row. If we were to use the labels from the earlier diagram, the weights would fit into the matrix in the following locations:\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/06d40e12-57c1-4a5a-b697-c00eb1d22bd1\"/>\n",
    "<center>Weight matrix shown with labels matching earlier diagrams.</center>\n",
    "\n",
    "Remember, the above is **not** a correct view of the **indices**, but it uses the labels from the earlier neural net diagrams to show you where each weight ends up in the matrix.\n",
    "\n",
    "The important thing with matrix multiplication is that the **dimensions match**. For matrix multiplication to work, there has to be the same number of elements in the dot products. In the first example, there are three columns in the input vector, and three rows in the weights matrix. In the second example, there are three columns in the weights matrix and three rows in the input vector. If the dimensions don't match, you'll get this:\n",
    "\n",
    "```python\n",
    "# Same weights and features as above, but swapped the order\n",
    "hidden_inputs = np.dot(weights_input_to_hidden, features)\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-11-1bfa0f615c45> in <module>()\n",
    "----> 1 hidden_in = np.dot(weights_input_to_hidden, X)\n",
    "\n",
    "ValueError: shapes (3,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)\n",
    "```\n",
    "The dot product can't be computed for a 3x2 matrix and 3-element array. That's because the 2 columns in the matrix don't match the number of elements in the array. Some of the dimensions that could work would be the following:\n",
    "<div align=center><img width=\"70%\" height=\"70%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/72b656f0-00d7-4750-a182-0b354d1d9705\"/>\n",
    "\n",
    "The rule is that if you're multiplying an array from the left, the array must have the same number of elements as there are rows in the matrix. And if you're multiplying the matrix from the left, the number of columns in the matrix must equal the number of elements in the array on the right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ab33df64-0217-4400-88fb-584e688fbaea"
    }
   },
   "source": [
    "## Making a column vector\n",
    "\n",
    "You see above that sometimes you'll want a column vector, even though by default Numpy arrays work like row vectors. It's possible to get the transpose of an array like so arr.T, but for a 1D array, the transpose will return a row vector. Instead, use arr[:,None] to create a column vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "75556aa0-1641-4041-a22a-25a3fb6b8b09"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[ 0.49671415 -0.1382643   0.64768854]\n",
      "[ 0.49671415 -0.1382643   0.64768854]\n",
      "[[ 0.49671415]\n",
      " [-0.1382643 ]\n",
      " [ 0.64768854]]\n"
     ]
    }
   ],
   "source": [
    "features = np.array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features.shape)\n",
    "\n",
    "print(features)\n",
    "\n",
    "print(features.T)\n",
    "\n",
    "print(features[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "46a8553a-50d9-4934-88d3-f99c9b91c86c"
    }
   },
   "source": [
    "Alternatively, you can create arrays with two dimensions. Then, you can use arr.T to get the column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "00f3e7cf-f89c-421f-b386-90da609fcd91"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49671415 -0.1382643   0.64768854]]\n",
      "[[ 0.49671415]\n",
      " [-0.1382643 ]\n",
      " [ 0.64768854]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(features, ndmin=2))\n",
    "print(np.array(features, ndmin=2).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "013b96bb-800b-481e-9408-c32fbbd2f74f"
    }
   },
   "source": [
    "## Programming quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bd556a4c-ec53-4d45-b359-3dc4eea1b0fd"
    }
   },
   "source": [
    "Below, you'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers.\n",
    "\n",
    "Things to do:\n",
    "\n",
    ">* Calculate the input to the hidden layer.\n",
    ">* Calculate the hidden layer output.\n",
    ">* Calculate the input to the output layer.\n",
    ">* Calculate the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6b1ed3f9-6135-478d-9b62-5355fa300ec4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[ 0.41492192  0.42604313  0.5002434 ]\n",
      "Output-layer Output:\n",
      "[ 0.49815196  0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "## 注意此处的np.dot(X, weights_input_to_hidden)\n",
    "# print X.shape\n",
    "# print weights_input_to_hidden.shape\n",
    "# print np.dot(X,weights_input_to_hidden)\n",
    "# (4,)\n",
    "# (4, 3)\n",
    "# [-0.34365494 -0.29801368  0.00097362]\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c6af3c7e-10c2-432b-9bf9-a5bdf8de052f"
    }
   },
   "source": [
    "# Backpropagation\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5667e3db-266e-48c4-8fb5-9078dc148fed"
    }
   },
   "source": [
    "**Backpropagation**\n",
    "Now we've come to the problem of how to make a multilayer neural network learn. Before, we saw how to update weights with gradient descent. The backpropagation algorithm is just an extension of that, using the chain rule to find the error with the respect to the weights connecting the input layer to the hidden layer (for a two layer network).\n",
    "\n",
    "To update the weights to hidden layers using gradient descent, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers.\n",
    "\n",
    "For example, in the output layer, you have errors $\\delta_{k}^{o}$ attributed to each output unit k. Then, the error attributed to hidden unit j is the output errors, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "\n",
    "$$\\delta_{j}^{h} = \\sum W_{jk}\\delta_{k}^{o}f'(h_{j})$$\n",
    "\n",
    "\n",
    "[**推导**](https://www.zybuluo.com/hanbingtao/note/476663)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78cf5d96-6965-4135-981a-53dfdf03327f"
    }
   },
   "source": [
    "Then, the gradient descent step is the same as before, just with the new errors:\n",
    "\n",
    "$$\\Delta w_{ij} = \\eta \\delta_{j}^{h}x_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7eb22e1d-1f05-4ca2-8d08-05fcf800a207"
    }
   },
   "source": [
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_{i}$ are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer\n",
    "\n",
    "$$\\Delta w_{pq} = \\eta \\delta_{output}V_{in}$$\n",
    "\n",
    "Here, you get the output error, $ \\delta_{output} $, by propagating the errors backwards from higher layers. And the input values, $V_{in}$ are the inputs to the layer, the hidden layer activations to the output unit for example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5a63f083-b858-464f-b8b0-7e1010b6542c"
    }
   },
   "source": [
    "## Working through an example\n",
    "\n",
    "Let's walk through the steps of calculating the weight updates for a simple two layer network. Suppose there are two input values, one hidden unit, and one output unit, with sigmoid activations on the hidden and output units. The following image depicts this network. (**Note**: the input values are shown as nodes at the bottom of the image, while the networks output value is shown as $\\widehat {y}$ at the top. The inputs themselves do not count as a layer, which is why this is considered a two layer network.)\n",
    "\n",
    "<div align=center><img width=\"50%\" height=\"50%\" src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/0624caba-3895-48b9-8db1-8404983e4b33\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6ec0f3bc-6545-4079-93c2-14135b5f085b"
    }
   },
   "source": [
    "Assume we're trying to fit some binary data and the target is y=1. We'll start with the forward pass, first calculating the input to the hidden unit\n",
    "\n",
    "$ h = \\sum_{i}w_{i}x_{i} = 0.1×0.4−0.2×0.3=−0.02$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fe73e7da-9e91-47ac-acfa-571c4b75e1b2"
    }
   },
   "source": [
    "and the output of the hidden unit\n",
    "\n",
    "$ a= f(h) = sigmoid (−0.02)=0.495$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8be3b5ed-4926-4246-b596-1db0fd5bc4fd"
    }
   },
   "source": [
    "Using this as the input to the output unit, the output of the network is\n",
    "\n",
    "$\\widehat {y} = f(W \\cdot a) = sigmoid(0.1×0.495)=0.512$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c8ac2baf-6925-4711-9e05-e122bd2e8457"
    }
   },
   "source": [
    "With the network output, we can start the backwards pass to calculate the weight updates for both layers. Using the fact that for the sigmoid function $ f'(W \\cdot a) = f(W \\cdot a)(1 - f(W \\cdot a)) $, the error term for the output unit is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "335bfc80-537c-4f54-932a-a78cffcb349b"
    }
   },
   "source": [
    "$\\delta^o = (y - \\widehat {y})f'(W \\cdot a) =(1−0.512)×0.512×(1−0.512)=0.122$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29079794-2b77-4094-b8ab-74277e6548bc"
    }
   },
   "source": [
    "Now we need to calculate the error term for the hidden unit with backpropagation. Here we'll scale the error term from the output unit by the weight **W** connecting it to the hidden unit. For the hidden unit error term, $ \\delta_{j}^{h} = \\sum_{k}W_{jk}\\delta_{k}^{o}f'(h_{j}) $, but since we have one hidden unit and one output unit, this is much simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "25d1abfa-8313-41b6-a001-9d228d0dd50c"
    }
   },
   "source": [
    "$\\delta^{h} = W\\delta^{o}f'(h) = 0.1×0.122×0.495×(1−0.495)=0.003$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f3bec3a4-ba91-4d8a-aacd-82aa3ca7d030"
    }
   },
   "source": [
    "Now that we have the errors, we can calculate the gradient descent steps. The hidden to output weight step is the learning rate, times the output unit error, times the hidden unit activation value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5a05ab2-3269-4b7a-a26e-8a820c2385ba"
    }
   },
   "source": [
    "$\\Delta W = \\eta \\delta^{o} a = 0.5×0.122×0.495=0.0302$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d3f1c8ee-14e6-42c1-ab84-fbf845a5fe96"
    }
   },
   "source": [
    "Then, for the input to hidden weights $w_{i}$ , it's the learning rate times the hidden unit error, times the input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8dfd9037-70bc-4369-9d23-28d026b60192"
    }
   },
   "source": [
    "$\\Delta w_{i} = \\eta \\delta^{h} x_{i} = (0.5×0.003×0.1,0.5×0.003×0.3)=(0.00015,0.00045)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a5adfdb0-cd61-4a9c-9619-bcf02cb6bc22"
    }
   },
   "source": [
    "From this example, you can see one of the effects of using the sigmoid function for the activations. **The maximum derivative of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%!** You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. This is known as the **vanishing gradient** problem. Later in the course you'll learn about other activation functions that perform better in this regard and are more commonly used in modern network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec611817-1233-42af-8527-28aab860e8e3"
    }
   },
   "source": [
    "## Implementing in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "843740c1-29f1-42c6-b9f6-19814f9afc06"
    }
   },
   "source": [
    "For the most part you have everything you need to implement backpropagation with Numpy.\n",
    "\n",
    "However, previously we were only dealing with error terms from one unit. Now, in the weight update, we have to consider the error for each unit in the hidden layer, $\\delta_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1e20dec6-965b-4ea5-8ffe-99b70f2392e4"
    }
   },
   "source": [
    "$\\Delta_{ij}=\\eta\\delta_{j}x_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a54bcb15-3769-41df-891b-8e0c1a61fdab"
    }
   },
   "source": [
    "Firstly, there will likely be a different number of input and hidden units, so trying to multiply the errors and the inputs as row vectors will throw an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8fbce37e-2138-457d-a681-a525c7d2efac"
    }
   },
   "source": [
    "```python\n",
    "hidden_error*inputs\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-22-3b59121cb809> in <module>()\n",
    "----> 1 hidden_error*x\n",
    "\n",
    "ValueError: operands could not be broadcast together with shapes (3,) (6,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b7986287-a4df-4416-933e-24710c8ed3a4"
    }
   },
   "source": [
    "Also, $w_{ij}$ is a matrix now, so the right side of the assignment must have the same shape as the left side. Luckily, Numpy takes care of this for us. If you multiply a row vector array with a column vector array, it will multiply the first element in the column by each element in the row vector and set that as the first row in a new 2D array. This continues for each element in the column vector, so you get a 2D array that has shape <font color=\"red\">(len(column_vector), len(row_vector))</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "37fe4113-5c73-4c56-978b-b2f6511b19e5"
    }
   },
   "source": [
    "```python\n",
    "hidden_error*inputs[:,None]\n",
    "array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],\n",
    "       [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],\n",
    "       [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "374352dd-3be5-4f10-bb27-df93eac40664"
    }
   },
   "source": [
    "It turns out this is exactly how we want to calculate the weight update step. As before, if you have your inputs as a 2D array with one row, you can also do **hidden_error*inputs**.T, but that won't work if **inputs** is a 1D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f19ba399-09c9-47b2-90ae-c794dc5903b7"
    }
   },
   "source": [
    "## Backpropagation exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "25f83ea9-1406-4dad-a304-761e66dfcc8c"
    }
   },
   "source": [
    "Below, you'll implement the code to calculate one backpropagation update step for two sets of weights. I wrote the forward pass, your goal is to code the backward pass.\n",
    "\n",
    "Things to do\n",
    "\n",
    ">* Calculate the network's output error.\n",
    ">* Calculate the output layer's error term.\n",
    ">* Use backpropagation to calculate the hidden layer's error term.\n",
    ">* Calculate the change in weights (the delta weights) that result from propagating the errors back through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9fd46da0-224c-46a7-8e7b-9284b501deaa"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * ( 1 - output)\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term,weights_hidden_output) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:,None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b1962323-d9a6-49da-9b69-7b0382bcd64c"
    }
   },
   "source": [
    "## 实现反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e2c0510-845a-4a56-ad6a-f341016e43f0"
    }
   },
   "source": [
    "现在我们知道输出层的误差是\n",
    "\n",
    "$\\delta_{k} = ( y_{k} - \\widehat y_{k} ) f'(a_{k})$\n",
    "\n",
    "输入层误差是\n",
    "\n",
    "$$\\delta_{j} = \\sum[w_{jk}\\delta_{k}] f'(h_{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ce21b79a-f3f0-48a3-a965-70e46b0cb0f1"
    }
   },
   "source": [
    "现在我们只考虑一个简单神经网络，他只有一层隐藏层和一个输出节点。这是通过反向传播更新权重的算法概述：\n",
    "\n",
    "- 把每一层权重更新的初始步长设置为 0\n",
    "    - 输入到隐藏层的权重是$\\Delta w_{ij}=0$\n",
    "    - 隐藏层到输出层的权重是$\\Delta W_{j}=0$\n",
    "- 对训练数据当中的每一个点\n",
    "    - 让它正向通过网络，计算输出$\\widehat y$\n",
    "    - 计算输出节点的误差梯度$\\delta^{o}=(y-\\widehat y)f'(z)$，这里的$z=\\sum_{j}W_{j}a_{j}$输入到输出节点\n",
    "    - 误差传播到隐藏层$\\delta_{j}^{h}=\\delta^{o}W_{j}f'(h_{j})$\n",
    "    - 更新权重步长:\n",
    "        - $\\Delta W_{j}=\\Delta W_{j}+\\delta^{o}a_{j}$\n",
    "        - $\\Delta w_{ij} = \\Delta w_{ij}+\\delta_{j}^{h}a_{i}$\n",
    "- 更新权重,$\\eta$是学习率，m是数据点数量:\n",
    "    - $W_{j} = W_{j} + \\eta\\Delta W_{j}/m$\n",
    "    - $w_{ij} = w_{ij} + \\eta\\Delta w_{ij}/m$\n",
    "- 重复这个过程 e 代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a42a9c3e-c90e-4098-ba10-9d8e5cbc4446"
    }
   },
   "source": [
    "### 反向传播练习\n",
    "现在你来实现一个通过反向传播训练的神经网络，数据集就是之前的研究生院录取数据。通过前面所学你现在有能力完成这个练习：\n",
    "\n",
    "你的目标是：\n",
    "\n",
    "- 实现一个正向传播\n",
    "- 实现反向传播\n",
    "- 更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "fdaa11b1-3ca5-4ba1-8519-ecde00851a30"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.25135725242598617)\n",
      "('Train loss: ', 0.24996540718842886)\n",
      "('Train loss: ', 0.24862005218904654)\n",
      "('Train loss: ', 0.24731993217179746)\n",
      "('Train loss: ', 0.24606380465584848)\n",
      "('Train loss: ', 0.24485044179257162)\n",
      "('Train loss: ', 0.2436786320186832)\n",
      "('Train loss: ', 0.24254718151769536)\n",
      "('Train loss: ', 0.24145491550165465)\n",
      "('Train loss: ', 0.24040067932493367)\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "#backprop.py\n",
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x,weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output,weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        #hidden_error = np.dot(output_error_term , weights_hidden_output)\n",
    "        hidden_error = output_error_term * weights_hidden_output\n",
    "        \n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:,None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "40dfbe6c-a7c2-4b8b-b012-ee2f9f3cbbcb"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "0569e572-15b6-4b22-a5a7-d2c78593efb0": {
     "id": "0569e572-15b6-4b22-a5a7-d2c78593efb0",
     "prev": "db7fa869-eb02-4365-8280-b1caf7395646",
     "regions": {
      "544ff509-01b1-4677-917d-d314d3545647": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fdaa11b1-3ca5-4ba1-8519-ecde00851a30",
        "part": "whole"
       },
       "id": "544ff509-01b1-4677-917d-d314d3545647"
      }
     }
    },
    "069bb17e-1a88-4b3f-8ef1-1d0196803694": {
     "id": "069bb17e-1a88-4b3f-8ef1-1d0196803694",
     "prev": "470cf196-6274-411e-b675-d2e7157c471f",
     "regions": {
      "5dd3e2be-0caa-466c-84a0-98b43d632499": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9173e3cf-ac24-48e4-a768-decaac7e7272",
        "part": "whole"
       },
       "id": "5dd3e2be-0caa-466c-84a0-98b43d632499"
      }
     }
    },
    "094918eb-e449-4636-90e3-e2e9b85115b2": {
     "id": "094918eb-e449-4636-90e3-e2e9b85115b2",
     "prev": "954c72f1-14cf-4603-9a3c-faa18251f85b",
     "regions": {
      "55d7f316-040b-4733-822f-9ff424f24988": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "25d1abfa-8313-41b6-a001-9d228d0dd50c",
        "part": "whole"
       },
       "id": "55d7f316-040b-4733-822f-9ff424f24988"
      }
     }
    },
    "0c36f3e3-cfb8-4e15-96a8-893e5c04f04d": {
     "id": "0c36f3e3-cfb8-4e15-96a8-893e5c04f04d",
     "prev": "4470c5ac-2bae-4155-ac02-3b3f6bfa6e27",
     "regions": {
      "9b68abc9-d307-4654-b7c3-1784e37ea3fe": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3ab4c514-12bb-40d8-9b2a-bc188cf5bbbf",
        "part": "whole"
       },
       "id": "9b68abc9-d307-4654-b7c3-1784e37ea3fe"
      }
     }
    },
    "0dcdb518-e55b-4cc4-badb-38d3cba7776e": {
     "id": "0dcdb518-e55b-4cc4-badb-38d3cba7776e",
     "prev": "5d855e1c-fe59-432c-9d62-d8876f6fcb45",
     "regions": {
      "4c446f4d-3bfb-42f2-a996-d6f8b025529f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6ec0f3bc-6545-4079-93c2-14135b5f085b",
        "part": "whole"
       },
       "id": "4c446f4d-3bfb-42f2-a996-d6f8b025529f"
      }
     }
    },
    "0eda9249-44b7-44d7-a140-43a3f24a2205": {
     "id": "0eda9249-44b7-44d7-a140-43a3f24a2205",
     "prev": "5d3e4a43-66b5-4db6-84c2-7483919fda3c",
     "regions": {
      "f03e602b-f2fb-407e-bf94-05c8dd0c2106": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5667e3db-266e-48c4-8fb5-9078dc148fed",
        "part": "whole"
       },
       "id": "f03e602b-f2fb-407e-bf94-05c8dd0c2106"
      }
     }
    },
    "107dc9a9-3a59-47f9-b1e6-9fae6b1fd936": {
     "id": "107dc9a9-3a59-47f9-b1e6-9fae6b1fd936",
     "prev": "9781bb6f-e719-4efa-bf13-25df9f613bc6",
     "regions": {
      "c168226c-8dd1-428d-928a-52d05057e3aa": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4dba093f-793b-475c-9a3e-500c6d9e6dba",
        "part": "whole"
       },
       "id": "c168226c-8dd1-428d-928a-52d05057e3aa"
      }
     }
    },
    "1884a533-634f-4eb6-8d61-cd97de97dd33": {
     "id": "1884a533-634f-4eb6-8d61-cd97de97dd33",
     "prev": "8fdf5174-009f-4c53-8a1f-3fa9371ec23b",
     "regions": {
      "d320fe9b-6e15-4521-b9b6-f3f3c7f98e43": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "00f3e7cf-f89c-421f-b386-90da609fcd91",
        "part": "whole"
       },
       "id": "d320fe9b-6e15-4521-b9b6-f3f3c7f98e43"
      }
     }
    },
    "1ceeecfb-1b26-42a4-b568-018e0a9a1559": {
     "id": "1ceeecfb-1b26-42a4-b568-018e0a9a1559",
     "prev": "b0916ecc-90c7-48f8-97ca-ca361d682c24",
     "regions": {
      "7c9b6336-3ed9-4d7d-a13e-9340d4b6aadc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c7bec236-b8e9-4079-a8e7-4113790b0b9c",
        "part": "whole"
       },
       "id": "7c9b6336-3ed9-4d7d-a13e-9340d4b6aadc"
      }
     }
    },
    "26c4bace-7f3f-4f13-a732-bf9e7778e3f6": {
     "id": "26c4bace-7f3f-4f13-a732-bf9e7778e3f6",
     "prev": "b82c9064-d917-46bd-84ef-d752948ec774",
     "regions": {
      "d98ddbf2-d280-4d9a-8097-710c1b9da5f3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3e483173-2226-4d50-bd63-633d3643fafe",
        "part": "whole"
       },
       "id": "d98ddbf2-d280-4d9a-8097-710c1b9da5f3"
      }
     }
    },
    "2d1b1777-5867-4b70-831c-9a906c0e74d0": {
     "id": "2d1b1777-5867-4b70-831c-9a906c0e74d0",
     "prev": "f29698bd-fbef-43e6-98f0-dffe91eb297c",
     "regions": {
      "1ad4de67-88fb-4455-99d5-832ab733da3f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "75556aa0-1641-4041-a22a-25a3fb6b8b09",
        "part": "whole"
       },
       "id": "1ad4de67-88fb-4455-99d5-832ab733da3f"
      }
     }
    },
    "2ea80378-459b-4267-b6f7-93a7c71dc4c5": {
     "id": "2ea80378-459b-4267-b6f7-93a7c71dc4c5",
     "prev": "0eda9249-44b7-44d7-a140-43a3f24a2205",
     "regions": {
      "ccd1208a-5cf7-4333-9130-a014f0cd01a8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "78cf5d96-6965-4135-981a-53dfdf03327f",
        "part": "whole"
       },
       "id": "ccd1208a-5cf7-4333-9130-a014f0cd01a8"
      }
     }
    },
    "315d1eaf-84b7-4040-99a3-15b5a5095eb9": {
     "id": "315d1eaf-84b7-4040-99a3-15b5a5095eb9",
     "prev": "547abf3f-c6b7-4099-a5b1-19d7f563932d",
     "regions": {
      "ad068df3-20a7-4ecc-ace3-8339a2df16cb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9bb0026c-fdd2-4237-bd60-0b9f2e7c626b",
        "part": "whole"
       },
       "id": "ad068df3-20a7-4ecc-ace3-8339a2df16cb"
      }
     }
    },
    "346b4149-a41f-49bc-9f5f-facf7f1fdaf8": {
     "id": "346b4149-a41f-49bc-9f5f-facf7f1fdaf8",
     "prev": "b1ec44e7-6cfa-4ef3-9632-9ce11245262b",
     "regions": {
      "9afdc540-becc-4201-8ae4-066e00961957": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9fd46da0-224c-46a7-8e7b-9284b501deaa",
        "part": "whole"
       },
       "id": "9afdc540-becc-4201-8ae4-066e00961957"
      }
     }
    },
    "34f5c106-2a87-45ee-ba86-7b9f9f81bb98": {
     "id": "34f5c106-2a87-45ee-ba86-7b9f9f81bb98",
     "prev": "820704b5-eede-4a7a-b031-7eaa70b6834b",
     "regions": {
      "4fbab194-4c71-48c4-b5eb-794ccf431ebc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9c63fe37-6a73-432f-a102-afe02a462b9f",
        "part": "whole"
       },
       "id": "4fbab194-4c71-48c4-b5eb-794ccf431ebc"
      }
     }
    },
    "37e4d099-2f50-457a-8ba7-5ba79a8fd48e": {
     "id": "37e4d099-2f50-457a-8ba7-5ba79a8fd48e",
     "prev": "f2c995d1-e5aa-4030-9c2b-c69e15a5fba2",
     "regions": {
      "62e17311-d3eb-474e-af7f-1cc1ebafaf08": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "335bfc80-537c-4f54-932a-a78cffcb349b",
        "part": "whole"
       },
       "id": "62e17311-d3eb-474e-af7f-1cc1ebafaf08"
      }
     }
    },
    "3fa9155e-eeae-4e96-8eee-38b4ce0ca46f": {
     "id": "3fa9155e-eeae-4e96-8eee-38b4ce0ca46f",
     "prev": "6aabdad2-0416-45af-b60a-c507c758f303",
     "regions": {
      "5f1eefe3-222f-4e34-bd03-e2f18d762ad5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fa9e906c-d45b-4361-a978-333301bdeed7",
        "part": "whole"
       },
       "id": "5f1eefe3-222f-4e34-bd03-e2f18d762ad5"
      }
     }
    },
    "4470c5ac-2bae-4155-ac02-3b3f6bfa6e27": {
     "id": "4470c5ac-2bae-4155-ac02-3b3f6bfa6e27",
     "prev": "26c4bace-7f3f-4f13-a732-bf9e7778e3f6",
     "regions": {
      "ee19c74e-e3e2-4fdf-964f-9a631a4bc98d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d540e9a3-9896-4c0a-aeb3-4d5f221e1aad",
        "part": "whole"
       },
       "id": "ee19c74e-e3e2-4fdf-964f-9a631a4bc98d"
      }
     }
    },
    "469b81b4-ff8f-42d7-8be6-ba56436444c2": {
     "id": "469b81b4-ff8f-42d7-8be6-ba56436444c2",
     "prev": "346b4149-a41f-49bc-9f5f-facf7f1fdaf8",
     "regions": {
      "53ecab61-6bb2-468f-bfc8-7ad50c61bcf7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b1962323-d9a6-49da-9b69-7b0382bcd64c",
        "part": "whole"
       },
       "id": "53ecab61-6bb2-468f-bfc8-7ad50c61bcf7"
      }
     }
    },
    "470cf196-6274-411e-b675-d2e7157c471f": {
     "id": "470cf196-6274-411e-b675-d2e7157c471f",
     "prev": "107dc9a9-3a59-47f9-b1e6-9fae6b1fd936",
     "regions": {
      "7d96ad47-889e-4b31-8a12-165e26c58ef9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ad222687-d182-407d-9b47-9ead195d195b",
        "part": "whole"
       },
       "id": "7d96ad47-889e-4b31-8a12-165e26c58ef9"
      }
     }
    },
    "483a0745-677e-4f75-bc44-3d175ea5e991": {
     "id": "483a0745-677e-4f75-bc44-3d175ea5e991",
     "prev": "e0e44423-adb9-46e9-aa8d-93888f1c7677",
     "regions": {
      "07c2c3df-f41b-4986-9ab0-7b92b3e8a253": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "87a883cd-7098-450b-a512-0a4422a55b07",
        "part": "whole"
       },
       "id": "07c2c3df-f41b-4986-9ab0-7b92b3e8a253"
      }
     }
    },
    "486aa17a-5e83-4bfb-aa63-7d28947c800e": {
     "id": "486aa17a-5e83-4bfb-aa63-7d28947c800e",
     "prev": null,
     "regions": {
      "4741cab0-ef4d-483a-b06a-cbd6f7d09993": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "24824560-0dba-4a71-8e35-ce73eb9649b6",
        "part": "whole"
       },
       "id": "4741cab0-ef4d-483a-b06a-cbd6f7d09993"
      }
     }
    },
    "4a394f25-0f34-4d9a-8d7b-951adf500d20": {
     "id": "4a394f25-0f34-4d9a-8d7b-951adf500d20",
     "prev": "f6c9aae6-34a9-43e2-9c51-4afb2d458d7e",
     "regions": {
      "e40f6bf0-7c90-4fe5-b1e7-566112084c38": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d3f1c8ee-14e6-42c1-ab84-fbf845a5fe96",
        "part": "whole"
       },
       "id": "e40f6bf0-7c90-4fe5-b1e7-566112084c38"
      }
     }
    },
    "52a274a5-680b-4dfa-ac24-1e48e84e2f2c": {
     "id": "52a274a5-680b-4dfa-ac24-1e48e84e2f2c",
     "prev": "cee48d60-325b-455d-b589-a9404e3b0cfd",
     "regions": {
      "ebf8c122-f5fa-4cf2-b183-89a7c4637db9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "843740c1-29f1-42c6-b9f6-19814f9afc06",
        "part": "whole"
       },
       "id": "ebf8c122-f5fa-4cf2-b183-89a7c4637db9"
      }
     }
    },
    "53478e1e-faee-4ab0-9f2d-ab0a0edc5a0d": {
     "id": "53478e1e-faee-4ab0-9f2d-ab0a0edc5a0d",
     "prev": "e3b45ab5-4168-4b91-a0c9-286f4cf20a6a",
     "regions": {
      "405d75e6-199a-4757-83b7-0f4f70eb0748": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a54bcb15-3769-41df-891b-8e0c1a61fdab",
        "part": "whole"
       },
       "id": "405d75e6-199a-4757-83b7-0f4f70eb0748"
      }
     }
    },
    "547abf3f-c6b7-4099-a5b1-19d7f563932d": {
     "id": "547abf3f-c6b7-4099-a5b1-19d7f563932d",
     "prev": "1ceeecfb-1b26-42a4-b568-018e0a9a1559",
     "regions": {
      "f425179c-dd77-4871-8422-743d48713723": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c6171a9c-7215-4dc4-9da9-c4cdf0a918fe",
        "part": "whole"
       },
       "id": "f425179c-dd77-4871-8422-743d48713723"
      }
     }
    },
    "591ac35b-b83f-4161-a599-9a00baa4d46b": {
     "id": "591ac35b-b83f-4161-a599-9a00baa4d46b",
     "prev": "483a0745-677e-4f75-bc44-3d175ea5e991",
     "regions": {
      "05f5ad4a-6f1c-4ed2-a330-ae5018dc897c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0f77a547-c32b-4ec7-a759-bf5349edcdc4",
        "part": "whole"
       },
       "id": "05f5ad4a-6f1c-4ed2-a330-ae5018dc897c"
      }
     }
    },
    "5a88f116-f9ca-4020-bb2c-af739cb889bd": {
     "id": "5a88f116-f9ca-4020-bb2c-af739cb889bd",
     "prev": "b3183402-e398-4f98-90c0-f609eb8ddd61",
     "regions": {
      "95a6a150-578a-4904-ab0e-52cce7ce0281": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7651fec6-9fa0-4331-993a-c8ac5a9c051d",
        "part": "whole"
       },
       "id": "95a6a150-578a-4904-ab0e-52cce7ce0281"
      }
     }
    },
    "5ac4ce7b-5bb1-4ed8-bff5-a833a69ba17e": {
     "id": "5ac4ce7b-5bb1-4ed8-bff5-a833a69ba17e",
     "prev": "34f5c106-2a87-45ee-ba86-7b9f9f81bb98",
     "regions": {
      "17b6cffc-74b0-4352-94bd-912e321eeea7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "735230e5-94f4-475f-80f4-7be267493035",
        "part": "whole"
       },
       "id": "17b6cffc-74b0-4352-94bd-912e321eeea7"
      }
     }
    },
    "5d3e4a43-66b5-4db6-84c2-7483919fda3c": {
     "id": "5d3e4a43-66b5-4db6-84c2-7483919fda3c",
     "prev": "ea47e3f7-a73b-4f80-bd60-e6c48449899e",
     "regions": {
      "78c9d5e4-171e-4588-9bd2-0921d91583c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c6af3c7e-10c2-432b-9bf9-a5bdf8de052f",
        "part": "whole"
       },
       "id": "78c9d5e4-171e-4588-9bd2-0921d91583c8"
      }
     }
    },
    "5d855e1c-fe59-432c-9d62-d8876f6fcb45": {
     "id": "5d855e1c-fe59-432c-9d62-d8876f6fcb45",
     "prev": "af49ecb8-a0a2-4c01-bbca-63e553f44558",
     "regions": {
      "03cfdf57-4b2d-418a-9a19-044d5714e0d6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5a63f083-b858-464f-b8b0-7e1010b6542c",
        "part": "whole"
       },
       "id": "03cfdf57-4b2d-418a-9a19-044d5714e0d6"
      }
     }
    },
    "5da7121d-9030-47f3-9356-91bf418defc3": {
     "id": "5da7121d-9030-47f3-9356-91bf418defc3",
     "prev": "0dcdb518-e55b-4cc4-badb-38d3cba7776e",
     "regions": {
      "d1490be0-2150-45c7-902c-df37a401df1e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe73e7da-9e91-47ac-acfa-571c4b75e1b2",
        "part": "whole"
       },
       "id": "d1490be0-2150-45c7-902c-df37a401df1e"
      }
     }
    },
    "61005ec2-75ad-4aae-bd42-6f2f467cd6f2": {
     "id": "61005ec2-75ad-4aae-bd42-6f2f467cd6f2",
     "prev": "315d1eaf-84b7-4040-99a3-15b5a5095eb9",
     "regions": {
      "3dbd057c-b236-4e1c-beee-28890061018f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a259dc8c-cb1c-4f82-b283-75880ae43a41",
        "part": "whole"
       },
       "id": "3dbd057c-b236-4e1c-beee-28890061018f"
      }
     }
    },
    "6aabdad2-0416-45af-b60a-c507c758f303": {
     "id": "6aabdad2-0416-45af-b60a-c507c758f303",
     "prev": "069bb17e-1a88-4b3f-8ef1-1d0196803694",
     "regions": {
      "88b769f7-efb6-4f67-8f6b-8de90776f42c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0c97b68b-7942-46e0-81c5-878f98dae209",
        "part": "whole"
       },
       "id": "88b769f7-efb6-4f67-8f6b-8de90776f42c"
      }
     }
    },
    "74d32ae7-b5f3-44a1-93ef-c1b20c4ff6ab": {
     "id": "74d32ae7-b5f3-44a1-93ef-c1b20c4ff6ab",
     "prev": "094918eb-e449-4636-90e3-e2e9b85115b2",
     "regions": {
      "699cb3a4-c97b-46de-bb79-d791f2e9df45": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3bec3a4-ba91-4d8a-aacd-82aa3ca7d030",
        "part": "whole"
       },
       "id": "699cb3a4-c97b-46de-bb79-d791f2e9df45"
      }
     }
    },
    "796f0bca-6768-4204-8d6b-73ab134a8ee7": {
     "id": "796f0bca-6768-4204-8d6b-73ab134a8ee7",
     "prev": "9d069c10-c4e9-4574-805e-14ae468f5eb5",
     "regions": {
      "f0d79e0e-09fa-4c4c-a488-c6c738b5c3c2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c3c2e62b-e799-41b1-84b3-bdedd9d393f4",
        "part": "whole"
       },
       "id": "f0d79e0e-09fa-4c4c-a488-c6c738b5c3c2"
      }
     }
    },
    "7a48b1b4-eef6-4d8c-a843-c7b9b9428825": {
     "id": "7a48b1b4-eef6-4d8c-a843-c7b9b9428825",
     "prev": "3fa9155e-eeae-4e96-8eee-38b4ce0ca46f",
     "regions": {
      "99417702-5c63-4674-8ac8-b38b9a8dd6f9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "84decc81-73a1-4c09-9669-96af9fd7bbfb",
        "part": "whole"
       },
       "id": "99417702-5c63-4674-8ac8-b38b9a8dd6f9"
      }
     }
    },
    "820704b5-eede-4a7a-b031-7eaa70b6834b": {
     "id": "820704b5-eede-4a7a-b031-7eaa70b6834b",
     "prev": "61005ec2-75ad-4aae-bd42-6f2f467cd6f2",
     "regions": {
      "14d71a40-179a-4a9a-8eb8-5a6d4861d7b2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "105dc666-8c84-413f-9161-bd1af1c7a577",
        "part": "whole"
       },
       "id": "14d71a40-179a-4a9a-8eb8-5a6d4861d7b2"
      }
     }
    },
    "83a47134-4598-456c-ab00-09bf070e57f6": {
     "id": "83a47134-4598-456c-ab00-09bf070e57f6",
     "prev": "edc16661-e038-4c9b-b326-13e2904658b1",
     "regions": {
      "10ae2792-7b03-45e0-ad52-f8670ff01b1f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "309c3d7c-aed4-4999-a2ed-6d102ca2120f",
        "part": "whole"
       },
       "id": "10ae2792-7b03-45e0-ad52-f8670ff01b1f"
      }
     }
    },
    "85635632-ff11-4e10-b28b-7151b2016b29": {
     "id": "85635632-ff11-4e10-b28b-7151b2016b29",
     "prev": "fbba1c19-8132-497e-90cd-27204aa4c337",
     "regions": {
      "5a358fa5-e781-4734-810e-80966694b304": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2e313dc4-9482-4a3c-904b-4bb43facd2d8",
        "part": "whole"
       },
       "id": "5a358fa5-e781-4734-810e-80966694b304"
      }
     }
    },
    "85a6d9ce-bc41-4130-866e-5e79eb0ab3a8": {
     "id": "85a6d9ce-bc41-4130-866e-5e79eb0ab3a8",
     "prev": "d3de29fa-9b06-49bb-83d6-9729378d7731",
     "regions": {
      "86511d2b-60c7-4422-b1f7-991fa338d61e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f72ded58-0c12-457b-8e95-ce3fceca6ec4",
        "part": "whole"
       },
       "id": "86511d2b-60c7-4422-b1f7-991fa338d61e"
      }
     }
    },
    "85ec2c13-56db-4514-b7b5-fac2ac8a502a": {
     "id": "85ec2c13-56db-4514-b7b5-fac2ac8a502a",
     "prev": "9ca447fb-6658-400e-9526-479fe59c21bf",
     "regions": {
      "348786c8-9105-48cf-bf31-f45eccfc6470": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "374352dd-3be5-4f10-bb27-df93eac40664",
        "part": "whole"
       },
       "id": "348786c8-9105-48cf-bf31-f45eccfc6470"
      }
     }
    },
    "8fa034a7-9101-4d3b-8732-b942d12f607c": {
     "id": "8fa034a7-9101-4d3b-8732-b942d12f607c",
     "prev": "486aa17a-5e83-4bfb-aa63-7d28947c800e",
     "regions": {
      "a2bae114-6c03-4a64-b65e-34107d65a56b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4ebc32ea-8e64-4622-9ec7-8e1fa60b64e5",
        "part": "whole"
       },
       "id": "a2bae114-6c03-4a64-b65e-34107d65a56b"
      }
     }
    },
    "8fdf5174-009f-4c53-8a1f-3fa9371ec23b": {
     "id": "8fdf5174-009f-4c53-8a1f-3fa9371ec23b",
     "prev": "2d1b1777-5867-4b70-831c-9a906c0e74d0",
     "regions": {
      "e2440272-0225-4b93-b4e9-55c0c1d9419a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "46a8553a-50d9-4934-88d3-f99c9b91c86c",
        "part": "whole"
       },
       "id": "e2440272-0225-4b93-b4e9-55c0c1d9419a"
      }
     }
    },
    "9203d9bc-00d8-4540-9b2f-69d61c989a99": {
     "id": "9203d9bc-00d8-4540-9b2f-69d61c989a99",
     "prev": "469b81b4-ff8f-42d7-8be6-ba56436444c2",
     "regions": {
      "bb33e1d2-eda8-400a-b6ba-ebed5bf8b20d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5e2c0510-845a-4a56-ad6a-f341016e43f0",
        "part": "whole"
       },
       "id": "bb33e1d2-eda8-400a-b6ba-ebed5bf8b20d"
      }
     }
    },
    "954c72f1-14cf-4603-9a3c-faa18251f85b": {
     "id": "954c72f1-14cf-4603-9a3c-faa18251f85b",
     "prev": "37e4d099-2f50-457a-8ba7-5ba79a8fd48e",
     "regions": {
      "fc11a824-b303-44f1-b63b-af06a6e67e93": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "29079794-2b77-4094-b8ab-74277e6548bc",
        "part": "whole"
       },
       "id": "fc11a824-b303-44f1-b63b-af06a6e67e93"
      }
     }
    },
    "9781bb6f-e719-4efa-bf13-25df9f613bc6": {
     "id": "9781bb6f-e719-4efa-bf13-25df9f613bc6",
     "prev": "f0478f02-d149-4db1-bb80-50a3bd1e25ae",
     "regions": {
      "0bf2c00c-f993-4b33-b73b-1fd790628a07": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9d8cfc3c-1b7b-414b-9816-6de552987d1f",
        "part": "whole"
       },
       "id": "0bf2c00c-f993-4b33-b73b-1fd790628a07"
      }
     }
    },
    "98e14f8b-4e48-4eab-bf39-6331540b7e41": {
     "id": "98e14f8b-4e48-4eab-bf39-6331540b7e41",
     "prev": "9203d9bc-00d8-4540-9b2f-69d61c989a99",
     "regions": {
      "7f1b8d9a-c583-476e-864e-1d5ee301bbad": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ce21b79a-f3f0-48a3-a965-70e46b0cb0f1",
        "part": "whole"
       },
       "id": "7f1b8d9a-c583-476e-864e-1d5ee301bbad"
      }
     }
    },
    "9ca447fb-6658-400e-9526-479fe59c21bf": {
     "id": "9ca447fb-6658-400e-9526-479fe59c21bf",
     "prev": "aa607d53-3f1d-4382-b8b7-d612c53b294c",
     "regions": {
      "497d120f-5807-4f74-a106-2a0a3cbb5293": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "37fe4113-5c73-4c56-978b-b2f6511b19e5",
        "part": "whole"
       },
       "id": "497d120f-5807-4f74-a106-2a0a3cbb5293"
      }
     }
    },
    "9d069c10-c4e9-4574-805e-14ae468f5eb5": {
     "id": "9d069c10-c4e9-4574-805e-14ae468f5eb5",
     "prev": "5ac4ce7b-5bb1-4ed8-bff5-a833a69ba17e",
     "regions": {
      "623b4a66-a9d3-4325-94e5-f03ff28fa49f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "db63c7dd-03d1-43e1-9989-3f76a4ceed83",
        "part": "whole"
       },
       "id": "623b4a66-a9d3-4325-94e5-f03ff28fa49f"
      }
     }
    },
    "a4861006-9a7c-48b7-9ff8-19c6203698bb": {
     "id": "a4861006-9a7c-48b7-9ff8-19c6203698bb",
     "prev": "5da7121d-9030-47f3-9356-91bf418defc3",
     "regions": {
      "8bef2249-50f1-4854-820e-f4701c29abea": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8be3b5ed-4926-4246-b596-1db0fd5bc4fd",
        "part": "whole"
       },
       "id": "8bef2249-50f1-4854-820e-f4701c29abea"
      }
     }
    },
    "aa607d53-3f1d-4382-b8b7-d612c53b294c": {
     "id": "aa607d53-3f1d-4382-b8b7-d612c53b294c",
     "prev": "f5fbc3f1-07e2-4979-aa82-0bcd29ff6495",
     "regions": {
      "46261b8f-b922-4b5b-a5b1-5331dca7ef1b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b7986287-a4df-4416-933e-24710c8ed3a4",
        "part": "whole"
       },
       "id": "46261b8f-b922-4b5b-a5b1-5331dca7ef1b"
      }
     }
    },
    "af49ecb8-a0a2-4c01-bbca-63e553f44558": {
     "id": "af49ecb8-a0a2-4c01-bbca-63e553f44558",
     "prev": "2ea80378-459b-4267-b6f7-93a7c71dc4c5",
     "regions": {
      "4d10e2e3-98e6-43f1-ac21-fffbaf730423": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7eb22e1d-1f05-4ca2-8d08-05fcf800a207",
        "part": "whole"
       },
       "id": "4d10e2e3-98e6-43f1-ac21-fffbaf730423"
      }
     }
    },
    "b0916ecc-90c7-48f8-97ca-ca361d682c24": {
     "id": "b0916ecc-90c7-48f8-97ca-ca361d682c24",
     "prev": "591ac35b-b83f-4161-a599-9a00baa4d46b",
     "regions": {
      "0dd43657-8862-403c-bd3b-fbcff5d07400": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "61a83d6e-0fef-466a-a57f-b588efdfa52f",
        "part": "whole"
       },
       "id": "0dd43657-8862-403c-bd3b-fbcff5d07400"
      }
     }
    },
    "b101cc47-a0ef-4f05-9d1e-ab081f28f21b": {
     "id": "b101cc47-a0ef-4f05-9d1e-ab081f28f21b",
     "prev": "85ec2c13-56db-4514-b7b5-fac2ac8a502a",
     "regions": {
      "58889ace-b879-40a8-b0aa-9e5a050f5f51": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f19ba399-09c9-47b2-90ae-c794dc5903b7",
        "part": "whole"
       },
       "id": "58889ace-b879-40a8-b0aa-9e5a050f5f51"
      }
     }
    },
    "b1ec44e7-6cfa-4ef3-9632-9ce11245262b": {
     "id": "b1ec44e7-6cfa-4ef3-9632-9ce11245262b",
     "prev": "b101cc47-a0ef-4f05-9d1e-ab081f28f21b",
     "regions": {
      "8cbd6777-f671-4e4f-9aff-0bf3b103d9e8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "25f83ea9-1406-4dad-a304-761e66dfcc8c",
        "part": "whole"
       },
       "id": "8cbd6777-f671-4e4f-9aff-0bf3b103d9e8"
      }
     }
    },
    "b3183402-e398-4f98-90c0-f609eb8ddd61": {
     "id": "b3183402-e398-4f98-90c0-f609eb8ddd61",
     "prev": "dc4078dc-d515-4e6b-843f-fcbb5ff1f26f",
     "regions": {
      "7dc61389-9942-4b51-885a-8321a0bd48e5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1ed00751-0004-420e-9509-477eaeded1b7",
        "part": "whole"
       },
       "id": "7dc61389-9942-4b51-885a-8321a0bd48e5"
      }
     }
    },
    "b82c9064-d917-46bd-84ef-d752948ec774": {
     "id": "b82c9064-d917-46bd-84ef-d752948ec774",
     "prev": "8fa034a7-9101-4d3b-8732-b942d12f607c",
     "regions": {
      "ee1a53c2-9a51-4b2d-b9bb-bc9e2df6ccdb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a6b44630-bd2e-46b9-a874-7029ac1dcb6b",
        "part": "whole"
       },
       "id": "ee1a53c2-9a51-4b2d-b9bb-bc9e2df6ccdb"
      }
     }
    },
    "b908b22b-169f-439b-a270-f0269d0f2560": {
     "id": "b908b22b-169f-439b-a270-f0269d0f2560",
     "prev": "0569e572-15b6-4b22-a5a7-d2c78593efb0",
     "regions": {
      "de07b946-3b0d-4ba6-a78d-6115f7ee5db7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "40dfbe6c-a7c2-4b8b-b012-ee2f9f3cbbcb",
        "part": "whole"
       },
       "id": "de07b946-3b0d-4ba6-a78d-6115f7ee5db7"
      }
     }
    },
    "bddb7352-b420-45e6-a6b4-836f8d697d8d": {
     "id": "bddb7352-b420-45e6-a6b4-836f8d697d8d",
     "prev": "efcbd54a-9fec-4d0d-b189-6a2ec179f78a",
     "regions": {
      "42d197d8-89e0-4631-8cf7-9576cdfdb2d2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bd556a4c-ec53-4d45-b359-3dc4eea1b0fd",
        "part": "whole"
       },
       "id": "42d197d8-89e0-4631-8cf7-9576cdfdb2d2"
      }
     }
    },
    "c56b5383-d322-42c6-b6f6-dbfdd7d6cce2": {
     "id": "c56b5383-d322-42c6-b6f6-dbfdd7d6cce2",
     "prev": "85635632-ff11-4e10-b28b-7151b2016b29",
     "regions": {
      "7d9766d6-0813-4c46-b4d9-0a69d8bd5baa": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4565ae43-5494-4398-ae2c-405e654fa342",
        "part": "whole"
       },
       "id": "7d9766d6-0813-4c46-b4d9-0a69d8bd5baa"
      }
     }
    },
    "c8969b3c-3a30-4e72-b1bb-bafdf88aea5e": {
     "id": "c8969b3c-3a30-4e72-b1bb-bafdf88aea5e",
     "prev": "5a88f116-f9ca-4020-bb2c-af739cb889bd",
     "regions": {
      "d176b33a-a128-4e20-aa7b-b903ac9d602f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "54f47273-09e6-4078-87fa-da364e41db1d",
        "part": "whole"
       },
       "id": "d176b33a-a128-4e20-aa7b-b903ac9d602f"
      }
     }
    },
    "cee48d60-325b-455d-b589-a9404e3b0cfd": {
     "id": "cee48d60-325b-455d-b589-a9404e3b0cfd",
     "prev": "dc7622e5-46d0-4e41-abd0-06af6d172965",
     "regions": {
      "d4124221-1e8b-4ff6-8169-fb8e5e47e6b2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ec611817-1233-42af-8527-28aab860e8e3",
        "part": "whole"
       },
       "id": "d4124221-1e8b-4ff6-8169-fb8e5e47e6b2"
      }
     }
    },
    "d0b4cea9-7b8f-4a48-bb99-5a67f1de42ef": {
     "id": "d0b4cea9-7b8f-4a48-bb99-5a67f1de42ef",
     "prev": "4a394f25-0f34-4d9a-8d7b-951adf500d20",
     "regions": {
      "248f6483-1a56-4416-a094-5106dd4e33f2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8dfd9037-70bc-4369-9d23-28d026b60192",
        "part": "whole"
       },
       "id": "248f6483-1a56-4416-a094-5106dd4e33f2"
      }
     }
    },
    "d3de29fa-9b06-49bb-83d6-9729378d7731": {
     "id": "d3de29fa-9b06-49bb-83d6-9729378d7731",
     "prev": "c8969b3c-3a30-4e72-b1bb-bafdf88aea5e",
     "regions": {
      "5b6fccbe-5545-4388-b3e4-a070cc97d528": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ee9bd2d2-6479-4b9d-b3e3-358076a0f27d",
        "part": "whole"
       },
       "id": "5b6fccbe-5545-4388-b3e4-a070cc97d528"
      }
     }
    },
    "db7fa869-eb02-4365-8280-b1caf7395646": {
     "id": "db7fa869-eb02-4365-8280-b1caf7395646",
     "prev": "98e14f8b-4e48-4eab-bf39-6331540b7e41",
     "regions": {
      "233e8c00-167b-4607-875a-839d89ff798a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a42a9c3e-c90e-4098-ba10-9d8e5cbc4446",
        "part": "whole"
       },
       "id": "233e8c00-167b-4607-875a-839d89ff798a"
      }
     }
    },
    "dc4078dc-d515-4e6b-843f-fcbb5ff1f26f": {
     "id": "dc4078dc-d515-4e6b-843f-fcbb5ff1f26f",
     "prev": "0c36f3e3-cfb8-4e15-96a8-893e5c04f04d",
     "regions": {
      "1e5e812e-3292-4a33-ae5c-8b7db1a83891": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "af7a5a01-69b4-41ea-bb23-8fe89b793723",
        "part": "whole"
       },
       "id": "1e5e812e-3292-4a33-ae5c-8b7db1a83891"
      }
     }
    },
    "dc7622e5-46d0-4e41-abd0-06af6d172965": {
     "id": "dc7622e5-46d0-4e41-abd0-06af6d172965",
     "prev": "d0b4cea9-7b8f-4a48-bb99-5a67f1de42ef",
     "regions": {
      "95383949-3400-43c7-bc22-432f53d0281c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a5adfdb0-cd61-4a9c-9619-bcf02cb6bc22",
        "part": "whole"
       },
       "id": "95383949-3400-43c7-bc22-432f53d0281c"
      }
     }
    },
    "e0e44423-adb9-46e9-aa8d-93888f1c7677": {
     "id": "e0e44423-adb9-46e9-aa8d-93888f1c7677",
     "prev": "85a6d9ce-bc41-4130-866e-5e79eb0ab3a8",
     "regions": {
      "78a9c305-1b59-4a34-87d6-0f903659c6b9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1315384e-25a0-4a73-ac9a-e3bc270bdfff",
        "part": "whole"
       },
       "id": "78a9c305-1b59-4a34-87d6-0f903659c6b9"
      }
     }
    },
    "e3b45ab5-4168-4b91-a0c9-286f4cf20a6a": {
     "id": "e3b45ab5-4168-4b91-a0c9-286f4cf20a6a",
     "prev": "52a274a5-680b-4dfa-ac24-1e48e84e2f2c",
     "regions": {
      "754749db-cf07-448b-b501-d46ffa751949": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1e20dec6-965b-4ea5-8ffe-99b70f2392e4",
        "part": "whole"
       },
       "id": "754749db-cf07-448b-b501-d46ffa751949"
      }
     }
    },
    "ea47e3f7-a73b-4f80-bd60-e6c48449899e": {
     "id": "ea47e3f7-a73b-4f80-bd60-e6c48449899e",
     "prev": "bddb7352-b420-45e6-a6b4-836f8d697d8d",
     "regions": {
      "da0dd764-9518-412f-81e7-ce36540b1d80": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6b1ed3f9-6135-478d-9b62-5355fa300ec4",
        "part": "whole"
       },
       "id": "da0dd764-9518-412f-81e7-ce36540b1d80"
      }
     }
    },
    "edc16661-e038-4c9b-b326-13e2904658b1": {
     "id": "edc16661-e038-4c9b-b326-13e2904658b1",
     "prev": "796f0bca-6768-4204-8d6b-73ab134a8ee7",
     "regions": {
      "6bd55726-9cf5-4b78-8d13-9c33e709e51d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3eee9e6a-b2de-423a-8db7-dd40cc08dd5e",
        "part": "whole"
       },
       "id": "6bd55726-9cf5-4b78-8d13-9c33e709e51d"
      }
     }
    },
    "efcbd54a-9fec-4d0d-b189-6a2ec179f78a": {
     "id": "efcbd54a-9fec-4d0d-b189-6a2ec179f78a",
     "prev": "1884a533-634f-4eb6-8d61-cd97de97dd33",
     "regions": {
      "e9e9809f-ed28-4475-9d26-8fc7758c20bb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "013b96bb-800b-481e-9408-c32fbbd2f74f",
        "part": "whole"
       },
       "id": "e9e9809f-ed28-4475-9d26-8fc7758c20bb"
      }
     }
    },
    "f0478f02-d149-4db1-bb80-50a3bd1e25ae": {
     "id": "f0478f02-d149-4db1-bb80-50a3bd1e25ae",
     "prev": "c56b5383-d322-42c6-b6f6-dbfdd7d6cce2",
     "regions": {
      "c73aa801-02c2-4b30-afdf-023cc16cd9f6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "154ec630-3a0c-4e8d-b0a7-3afa7b53bb5b",
        "part": "whole"
       },
       "id": "c73aa801-02c2-4b30-afdf-023cc16cd9f6"
      }
     }
    },
    "f29698bd-fbef-43e6-98f0-dffe91eb297c": {
     "id": "f29698bd-fbef-43e6-98f0-dffe91eb297c",
     "prev": "7a48b1b4-eef6-4d8c-a843-c7b9b9428825",
     "regions": {
      "55115e43-0fb2-4f69-aec2-6ef1f4cba57f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ab33df64-0217-4400-88fb-584e688fbaea",
        "part": "whole"
       },
       "id": "55115e43-0fb2-4f69-aec2-6ef1f4cba57f"
      }
     }
    },
    "f2c995d1-e5aa-4030-9c2b-c69e15a5fba2": {
     "id": "f2c995d1-e5aa-4030-9c2b-c69e15a5fba2",
     "prev": "a4861006-9a7c-48b7-9ff8-19c6203698bb",
     "regions": {
      "aae539ce-835f-494f-99ed-bfadcbd0566c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c8ac2baf-6925-4711-9e05-e122bd2e8457",
        "part": "whole"
       },
       "id": "aae539ce-835f-494f-99ed-bfadcbd0566c"
      }
     }
    },
    "f5fbc3f1-07e2-4979-aa82-0bcd29ff6495": {
     "id": "f5fbc3f1-07e2-4979-aa82-0bcd29ff6495",
     "prev": "53478e1e-faee-4ab0-9f2d-ab0a0edc5a0d",
     "regions": {
      "abe75585-a109-413a-b389-619dba1f5677": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8fbce37e-2138-457d-a681-a525c7d2efac",
        "part": "whole"
       },
       "id": "abe75585-a109-413a-b389-619dba1f5677"
      }
     }
    },
    "f6c9aae6-34a9-43e2-9c51-4afb2d458d7e": {
     "id": "f6c9aae6-34a9-43e2-9c51-4afb2d458d7e",
     "prev": "74d32ae7-b5f3-44a1-93ef-c1b20c4ff6ab",
     "regions": {
      "d714f1e8-1ed9-4282-bc7c-98eba7347f76": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d5a05ab2-3269-4b7a-a26e-8a820c2385ba",
        "part": "whole"
       },
       "id": "d714f1e8-1ed9-4282-bc7c-98eba7347f76"
      }
     }
    },
    "fbba1c19-8132-497e-90cd-27204aa4c337": {
     "id": "fbba1c19-8132-497e-90cd-27204aa4c337",
     "prev": "83a47134-4598-456c-ab00-09bf070e57f6",
     "regions": {
      "0c3a2ed5-ee2b-4818-839f-6f0c89d47057": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "401ecf92-3890-4c92-bd82-b219ae5b7096",
        "part": "whole"
       },
       "id": "0c3a2ed5-ee2b-4818-839f-6f0c89d47057"
      }
     }
    }
   },
   "themes": {
    "default": "9fbc5d9e-e3d1-4fb0-916e-c7fd365b97f8",
    "theme": {
     "52a6c5a2-6102-4a82-9d2e-229a792fdd13": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "52a6c5a2-6102-4a82-9d2e-229a792fdd13",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         253,
         246,
         227
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         88,
         110,
         117
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         38,
         139,
         210
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         101,
         123,
         131
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "9fbc5d9e-e3d1-4fb0-916e-c7fd365b97f8": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "9fbc5d9e-e3d1-4fb0-916e-c7fd365b97f8",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         0,
         43,
         54
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         232,
         213
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         38,
         139,
         210
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         147,
         161,
         161
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "434px",
    "left": "0px",
    "right": "1068px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
